{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4843172",
   "metadata": {},
   "source": [
    "Basic RAG 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8b222",
   "metadata": {},
   "source": [
    "1.Loading Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f6e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"Prompt Engineering.pdf\"\n",
    "\n",
    "MODEL = \"llama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dec610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages loaded: 68\n",
      "Length of a page : 281\n",
      "Content of a page: Prompt Engineering\n",
      "February 2025\n",
      "11\n",
      "• Top-P sampling selects the top tokens whose cumulative probability does not exceed \n",
      "a certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the \n",
      "LLM’s vocabulary).\n",
      "The best way to choose between top-K and top-P is to experiment with both methods (or \n",
      "both together) and see which one produces the results you are looking for. \n",
      "Putting it all together\n",
      "Choosing between top-K, top-P, temperature, and the number of tokens to generate, \n",
      "depends on the specific application and desired outcome, and the settings all impact one \n",
      "another. It’s also important to make sure you understand how your chosen model combines \n",
      "the different sampling settings together.\n",
      "If temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet \n",
      "both the top-K and top-P criteria are candidates for the next predicted token, and then \n",
      "temperature is applied to sample from the tokens that passed the top-K and top-P criteria. If \n",
      "only top-K or top-P is available, the behavior is the same but only the one top-K or P setting \n",
      "is used. \n",
      "If temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then \n",
      "randomly selected from to produce a single next predicted token.\n",
      "At extreme settings of one sampling configuration value, that one sampling setting either \n",
      "cancels out other configuration settings or becomes irrelevant.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(PDF_FILE)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Number of pages loaded: {len(pages)}\")\n",
    "print(f\"Length of a page : {len(pages[1].page_content)}\")\n",
    "print(\"Content of a page:\", pages[10].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a557320",
   "metadata": {},
   "source": [
    "2.Text Chunking with RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e1956f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 89\n",
      "Length of a chunk: 281\n",
      "Content of a chunk: Prompt Engineering\n",
      "February 2025\n",
      "10\n",
      "deterministic: the highest probability token is always selected (though note that if two tokens \n",
      "have the same highest predicted probability, depending on how tiebreaking is implemented \n",
      "you may not always get the same output with temperature 0).\n",
      "Temperatures close to the max tend to create more random output. And as temperature gets \n",
      "higher and higher, all tokens become equally likely to be the next predicted token.\n",
      "The Gemini temperature control can be understood in a similar way to the softmax function \n",
      "used in machine learning. A low temperature setting mirrors a low softmax temperature (T), \n",
      "emphasizing a single, preferred temperature with high certainty. A higher Gemini temperature \n",
      "setting is like a high softmax temperature, making a wider range of temperatures around \n",
      "the selected setting more acceptable. This increased uncertainty accommodates scenarios \n",
      "where a rigid, precise temperature may not be essential like for example when experimenting \n",
      "with creative outputs.\n",
      "Top-K and top-P\n",
      "Top-K and top-P (also known as nucleus sampling)4 are two sampling settings used in LLMs \n",
      "to restrict the predicted next token to come from tokens with the top predicted probabilities.  \n",
      "Like temperature, these sampling settings control the randomness and diversity of \n",
      "generated text.\n",
      "• Top-K sampling selects the top K most likely tokens from the model’s predicted \n",
      "distribution. The higher top-K, the more creative and varied the model’s output; the\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "print(\"Content of a chunk:\", chunks[10].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7331145",
   "metadata": {},
   "source": [
    "3.FAISS + Ollama Embedding Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e80dc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2662ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a95abc19-f984-47b0-896b-02ed4c0e8a11', metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'Prompt Engineering.pdf', 'total_pages': 68, 'page': 67, 'page_label': '68'}, page_content='10. Google Cloud Platform, 2023, Chain of Thought and React. Available at: https://github.com/ \\nGoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/chain_of_thought_react.ipynb . \\n11. Wang, X., et al., 2023, Self Consistency Improves Chain of Thought reasoning in language models.  \\nAvailable at: https://arxiv.org/pdf/2203.11171.pdf .\\n12. Yao, S., et al., 2023, Tree of Thoughts: Deliberate Problem Solving with Large Language Models.  \\nAvailable at: https://arxiv.org/pdf/2305.10601.pdf .\\n13. Yao, S., et al., 2023, ReAct: Synergizing Reasoning and Acting in Language Models. Available at:  \\nhttps://arxiv.org/pdf/2210.03629.pdf.\\n14. Google Cloud Platform, 2023, Advance Prompting: Chain of Thought and React. Available at: \\nhttps://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-  \\non-vertex-ai/advanced_prompting_training/cot_react.ipynb .\\n15. Zhou, C., et al., 2023, Automatic Prompt Engineering - Large Language Models are Human-Level Prompt \\nEngineers. Available at: https://arxiv.org/pdf/2211.01910.pdf .'),\n",
       " Document(id='ed9e17c0-3f36-4a55-ad93-71a5abd3c82c', metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'Prompt Engineering.pdf', 'total_pages': 68, 'page': 0, 'page_label': '1'}, page_content='Prompt  \\nEngineering\\nAuthor: Lee Boonstra'),\n",
       " Document(id='41e14836-1844-42d0-a112-764ceaffe564', metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'Prompt Engineering.pdf', 'total_pages': 68, 'page': 67, 'page_label': '68'}, page_content='Prompt Engineering\\nFebruary 2025\\n68\\nEndnotes\\n1. Google, 2023, Gemini by Google. Available at: https://gemini.google.com .\\n2. Google, 2024, Gemini for Google Workspace Prompt Guide. Available at:  \\nhttps://inthecloud.withgoogle.com/gemini-for-google-workspace-prompt-guide/dl-cd.html .\\n3. Google Cloud, 2023, Introduction to Prompting. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design .\\n4. Google Cloud, 2023, Text Model Request Body: Top-P & top-K sampling methods. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#request_body .\\n5. Wei, J., et al., 2023, Zero Shot - Fine Tuned language models are zero shot learners. Available at:  \\nhttps://arxiv.org/pdf/2109.01652.pdf .\\n6. Google Cloud, 2023, Google Cloud Model Garden. Available at: https://cloud.google.com/model-garden .\\n7. Brown, T., et al., 2023, Few Shot - Language Models are Few Shot learners. Available at:  \\nhttps://arxiv.org/pdf/2005.14165.pdf.\\n8. Zheng, L., et al., 2023, Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. \\nAvailable at: https://openreview.net/pdf?id=3bq3jsvcQ1\\n9. Wei, J., et al., 2023, Chain of Thought Prompting. Available at: https://arxiv.org/pdf/2201.11903.pdf .\\n10. Google Cloud Platform, 2023, Chain of Thought and React. Available at: https://github.com/ \\nGoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/chain_of_thought_react.ipynb .'),\n",
       " Document(id='e514b1b5-c4df-405e-8f43-3c9d1c699ef8', metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': 'Prompt Engineering.pdf', 'total_pages': 68, 'page': 15, 'page_label': '16'}, page_content='Prompt Engineering\\nFebruary 2025\\n16\\nThe number of examples you need for few-shot prompting depends on a few factors, \\nincluding the complexity of the task, the quality of the examples, and the capabilities of the \\ngenerative AI (gen AI) model you are using. As a general rule of thumb, you should use at \\nleast three to five examples for few-shot prompting. However, you may need to use more \\nexamples for more complex tasks, or you may need to use fewer due to the input length \\nlimitation of your model.\\nTable 2 shows a few-shot prompt example, let’s use the same gemini-pro model \\nconfiguration settings as before, other than increasing the token limit to accommodate the \\nneed for a longer response.\\nGoal Parse pizza orders to JSON\\nModel gemini-pro\\nTemperature 0.1 Token Limit 250\\nTop-K N/A Top-P 1\\nPrompt Parse a customer\\'s pizza order into valid JSON:\\nEXAMPLE:\\nI want a small pizza with cheese, tomato sauce, and pepperoni.\\nJSON Response:\\n```\\n{\\n\"size\": \"small\",\\n\"type\": \"normal\",\\n\"ingredients\": [[\"cheese\", \"tomato sauce\", \"peperoni\"]]\\n}\\n```\\nContinues next page...')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"what is Prompt Engineering?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345cabff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPrompt engineering is a relatively new subfield of natural language processing (NLP) that focuses on the design and optimization of language prompts or inputs to improve the performance of AI models in various tasks. The goal of prompt engineering is to create more effective and efficient language prompts that can elicit better responses from AI models, such as improved accuracy, relevance, and completeness.\\n\\nPrompt engineering involves a range of techniques, including:\\n\\n1. Designing optimal prompts: This involves creating prompts that are specifically tailored to the task at hand, using techniques such as template-based prompts, semantic prompts, and hybrid prompts.\\n2. Optimizing prompts for specific models: Prompt engineering can involve optimizing prompts for specific AI models, such as language translation models or question answering systems, to improve their performance on particular tasks.\\n3. Generating diverse prompts: Prompt engineering can also involve generating a diverse set of prompts to increase the chances of eliciting a relevant and accurate response from an AI model.\\n4. Evaluating prompt effectiveness: Prompt engineers use various evaluation metrics, such as accuracy, relevance, and completeness, to assess the effectiveness of prompts and identify areas for improvement.\\n\\nSome of the key applications of prompt engineering include:\\n\\n1. Improving language translation accuracy: By optimizing prompts for language translation models, prompt engineers can improve the accuracy and fluency of translations.\\n2. Enhancing question answering performance: Prompt engineering can help optimize prompts for question answering systems to improve their ability to provide accurate and relevant answers.\\n3. Generating creative content: Prompt engineering can be used to generate creative content, such as stories or poems, by designing optimal prompts that elicit specific responses from language models.\\n4. Improving dialogue systems: By optimizing prompts for dialogue systems, prompt engineers can improve the relevance and coherence of responses in conversations.\\n\\nOverall, prompt engineering is a rapidly evolving field that has the potential to significantly improve the performance of AI models in various natural language processing tasks.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Single Prompt Query on OllamaLLM (Testing)\n",
    "from langchain_ollama import OllamaLLM\n",
    "model = OllamaLLM(model=MODEL, temperature=0.1)\n",
    "model.invoke(\"What is Prompt Engineering?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cb836",
   "metadata": {},
   "source": [
    "4.Creating a Custom Prompt Template with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad9206dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides answers to questions based on\n",
      "a given context. \n",
      "\n",
      "Answer the question based on the context. If you can't answer the\n",
      "question, reply \"I don't know\".\n",
      "\n",
      "Be as concise as possible and go straight to the point.\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on\n",
    "a given context. \n",
    "\n",
    "Answer the question based on the context. If you can't answer the\n",
    "question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "394af286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sure! Based on the context you provided, Susan's sister is Anna.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Anna's sister is Susan\", \n",
    "    \"question\": \"Who is Susan's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f406976",
   "metadata": {},
   "source": [
    "5.Chaining Retriever, Prompt, and LLM in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "955d324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4f32f",
   "metadata": {},
   "source": [
    "6.Batch Inference on RAG Chain with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69bd5668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Top-K and Top-P?\n",
      "Answer:  Based on the provided context, I can answer your question. Top-K and Top-P are terms used in the field of prompt engineering for language models.\n",
      "\n",
      "Top-K refers to the number of examples or instances that a model is trained on. For instance, in the document you provided, Top-K is mentioned as \"N/A\" for the given prompt. This means that the model has not been trained on any specific number of examples for this task.\n",
      "\n",
      "Top-P, on the other hand, refers to the proportion of positive examples in a dataset. In the context of few-shot learning, Top-P represents the percentage of instances in the training set that are positive (i.e., belonging to the target class). A higher Top-P value indicates that the model has more opportunities to learn from positive examples, which can improve its performance on unseen data.\n",
      "\n",
      "In summary, Top-K refers to the number of examples trained on, while Top-P represents the proportion of positive instances in the training set.\n",
      "*************************\n",
      "\n",
      "Question: What is Temperature?\n",
      "Answer:  Based on the provided context, I can answer your question. Temperature is mentioned in the document as a parameter in the gemini-pro model configuration settings. Specifically, it is set to 0.1 in the example provided.\n",
      "*************************\n",
      "\n",
      "Question: What is Output Length?\n",
      "Answer:  Based on the provided context, the answer to the question \"What is Output Length?\" is:\n",
      "\n",
      "Output Length refers to the number of tokens or words in the output generated by a language model. It is an important metric in prompt engineering as it can affect the accuracy and relevance of the model's responses. A longer output length can provide more detailed and informative responses, but may also increase the risk of ambiguity or inaccuracy.\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is Top-K and Top-P?\",\n",
    "    \"What is Temperature?\",\n",
    "    \"What is Output Length?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print(\"*************************\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
